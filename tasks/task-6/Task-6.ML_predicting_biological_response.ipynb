{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning. Hyper Parameters Optimizaion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data for this task has been colleted experimentally and it represents a biological process. It contains the following columns:\n",
    "* Activity: actual biological response [0, 1]\n",
    "* D1-D1776: molecular descriptors like size, form and chemical elements.\n",
    "\n",
    "Data preprocessing is not required, data is already encoded and normalized.\n",
    "\n",
    "F1-score metric must be used within the task.\n",
    "\n",
    "Two models must be trained: logistic regression and random forest. Further a hyper parameter optimization must be performed using basic and advanced methods (GridSearchCV, RandomizedSearchCV, Hyperopt, Optuna). Maximum number of iterations must not exceed 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "from hyperopt import hp\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 10000\n",
    "n_splits = 5\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activity</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>...</th>\n",
       "      <th>D1767</th>\n",
       "      <th>D1768</th>\n",
       "      <th>D1769</th>\n",
       "      <th>D1770</th>\n",
       "      <th>D1771</th>\n",
       "      <th>D1772</th>\n",
       "      <th>D1773</th>\n",
       "      <th>D1774</th>\n",
       "      <th>D1775</th>\n",
       "      <th>D1776</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.497009</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132956</td>\n",
       "      <td>0.678031</td>\n",
       "      <td>0.273166</td>\n",
       "      <td>0.585445</td>\n",
       "      <td>0.743663</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.606291</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111209</td>\n",
       "      <td>0.803455</td>\n",
       "      <td>0.106105</td>\n",
       "      <td>0.411754</td>\n",
       "      <td>0.836582</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.480124</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.209791</td>\n",
       "      <td>0.610350</td>\n",
       "      <td>0.356453</td>\n",
       "      <td>0.517720</td>\n",
       "      <td>0.679051</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538825</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.196344</td>\n",
       "      <td>0.724230</td>\n",
       "      <td>0.235606</td>\n",
       "      <td>0.288764</td>\n",
       "      <td>0.805110</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.517794</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.494734</td>\n",
       "      <td>0.781422</td>\n",
       "      <td>0.154361</td>\n",
       "      <td>0.303809</td>\n",
       "      <td>0.812646</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3746</th>\n",
       "      <td>1</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.506409</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.209887</td>\n",
       "      <td>0.633426</td>\n",
       "      <td>0.297659</td>\n",
       "      <td>0.376124</td>\n",
       "      <td>0.727093</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>1</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.651023</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.151154</td>\n",
       "      <td>0.766505</td>\n",
       "      <td>0.170876</td>\n",
       "      <td>0.404546</td>\n",
       "      <td>0.787935</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.520564</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.179949</td>\n",
       "      <td>0.768785</td>\n",
       "      <td>0.177341</td>\n",
       "      <td>0.471179</td>\n",
       "      <td>0.872241</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.765646</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.536954</td>\n",
       "      <td>0.634936</td>\n",
       "      <td>0.342713</td>\n",
       "      <td>0.447162</td>\n",
       "      <td>0.672689</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.533952</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.347966</td>\n",
       "      <td>0.757971</td>\n",
       "      <td>0.230667</td>\n",
       "      <td>0.272652</td>\n",
       "      <td>0.854116</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3751 rows × 1777 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Activity        D1        D2    D3   D4        D5        D6        D7  \\\n",
       "0            1  0.000000  0.497009  0.10  0.0  0.132956  0.678031  0.273166   \n",
       "1            1  0.366667  0.606291  0.05  0.0  0.111209  0.803455  0.106105   \n",
       "2            1  0.033300  0.480124  0.00  0.0  0.209791  0.610350  0.356453   \n",
       "3            1  0.000000  0.538825  0.00  0.5  0.196344  0.724230  0.235606   \n",
       "4            0  0.100000  0.517794  0.00  0.0  0.494734  0.781422  0.154361   \n",
       "...        ...       ...       ...   ...  ...       ...       ...       ...   \n",
       "3746         1  0.033300  0.506409  0.10  0.0  0.209887  0.633426  0.297659   \n",
       "3747         1  0.133333  0.651023  0.15  0.0  0.151154  0.766505  0.170876   \n",
       "3748         0  0.200000  0.520564  0.00  0.0  0.179949  0.768785  0.177341   \n",
       "3749         1  0.100000  0.765646  0.00  0.0  0.536954  0.634936  0.342713   \n",
       "3750         0  0.133333  0.533952  0.00  0.0  0.347966  0.757971  0.230667   \n",
       "\n",
       "            D8        D9  ...  D1767  D1768  D1769  D1770  D1771  D1772  \\\n",
       "0     0.585445  0.743663  ...      0      0      0      0      0      0   \n",
       "1     0.411754  0.836582  ...      1      1      1      1      0      1   \n",
       "2     0.517720  0.679051  ...      0      0      0      0      0      0   \n",
       "3     0.288764  0.805110  ...      0      0      0      0      0      0   \n",
       "4     0.303809  0.812646  ...      0      0      0      0      0      0   \n",
       "...        ...       ...  ...    ...    ...    ...    ...    ...    ...   \n",
       "3746  0.376124  0.727093  ...      0      0      0      0      0      0   \n",
       "3747  0.404546  0.787935  ...      0      0      1      0      1      0   \n",
       "3748  0.471179  0.872241  ...      0      0      0      0      0      0   \n",
       "3749  0.447162  0.672689  ...      0      0      0      0      0      0   \n",
       "3750  0.272652  0.854116  ...      0      0      0      0      0      0   \n",
       "\n",
       "      D1773  D1774  D1775  D1776  \n",
       "0         0      0      0      0  \n",
       "1         0      0      1      0  \n",
       "2         0      0      0      0  \n",
       "3         0      0      0      0  \n",
       "4         0      0      0      0  \n",
       "...     ...    ...    ...    ...  \n",
       "3746      0      0      0      0  \n",
       "3747      1      0      0      0  \n",
       "3748      0      0      0      0  \n",
       "3749      0      0      0      0  \n",
       "3750      0      0      0      0  \n",
       "\n",
       "[3751 rows x 1777 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../../data/_train_sem09.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Activity', axis=1)\n",
    "y = data['Activity']\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Initial Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33 s, sys: 251 ms, total: 33.3 s\n",
      "Wall time: 3.82 s\n",
      "Train mean f1-score: 0.89\n",
      "Test mean f1-score: 0.78\n"
     ]
    }
   ],
   "source": [
    "lr_model = linear_model.LogisticRegression(random_state=random_state, max_iter=max_iter)\n",
    "\n",
    "kf = model_selection.KFold(n_splits=n_splits)\n",
    "\n",
    "%time cv_metrics = model_selection.cross_validate(estimator=lr_model, X=X, y=y, cv=kf, scoring='f1', return_train_score=True)\n",
    "\n",
    "print(f'Train mean f1-score: {np.mean(cv_metrics['train_score']):.2f}')\n",
    "print(f'Test mean f1-score: {np.mean(cv_metrics['test_score']):.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.54 s, sys: 216 ms, total: 8.75 s\n",
      "Wall time: 7.28 s\n",
      "Train mean f1-score: 1.00\n",
      "Test mean f1-score: 0.81\n"
     ]
    }
   ],
   "source": [
    "rf_model = ensemble.RandomForestClassifier(random_state=random_state)\n",
    "\n",
    "kf = model_selection.KFold(n_splits=n_splits)\n",
    "\n",
    "%time cv_metrics = model_selection.cross_validate(estimator=rf_model, X=X, y=y, cv=kf, scoring='f1', return_train_score=True)\n",
    "\n",
    "print(f'Train mean f1-score: {np.mean(cv_metrics['train_score']):.2f}')\n",
    "print(f'Test mean f1-score: {np.mean(cv_metrics['test_score']):.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Hyper Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 628 ms, sys: 284 ms, total: 912 ms\n",
      "Wall time: 3min 18s\n",
      "Train f1-score: 0.84\n",
      "Test f1-score: 0.80\n",
      "Best parameters:  {'C': 0.34, 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "# Total combinations: 1 * 2 * 4 + 1 * 6 * 4 = 32 <= 50\n",
    "param_grid = [\n",
    "    {\n",
    "        'penalty': ['l1'],\n",
    "        'solver': ['liblinear', 'saga'],\n",
    "        'C': list(np.linspace(0.01, 1.0, 4, dtype=float))\n",
    "    },\n",
    "    {\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['liblinear', 'lbfgs', 'newton-cg', 'newton-cholesky', 'saga', 'sag'],\n",
    "        'C': list(np.linspace(0.01, 1.0, 4, dtype=float))\n",
    "    }\n",
    "]\n",
    "\n",
    "grid_search_lr = model_selection.GridSearchCV(\n",
    "    estimator=linear_model.LogisticRegression(random_state=random_state, max_iter=max_iter),\n",
    "    param_grid=param_grid,\n",
    "    cv=n_splits,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "%time grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = grid_search_lr.predict(X_train)\n",
    "print(f'Train f1-score: {metrics.f1_score(y_train, y_train_pred):.2f}')\n",
    "\n",
    "y_test_pred = grid_search_lr.predict(X_test)\n",
    "print(f'Test f1-score: {metrics.f1_score(y_test, y_test_pred):.2f}')\n",
    "\n",
    "print('Best parameters: ', grid_search_lr.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.32 s, sys: 203 ms, total: 1.52 s\n",
      "Wall time: 32.9 s\n",
      "Train f1-score: 0.94\n",
      "Test f1-score: 0.83\n",
      "Best parameters:  {'max_depth': 20, 'min_samples_leaf': 5, 'n_estimators': 80}\n"
     ]
    }
   ],
   "source": [
    "# Total combinations: 4 * 2 * 6 = 48 <= 50\n",
    "param_grid = {\n",
    "    'n_estimators': list(range(80, 200, 30)),\n",
    "    'min_samples_leaf': [5, 10],\n",
    "    'max_depth': list(np.linspace(20, 40, 6, dtype=int))\n",
    "}\n",
    "\n",
    "grid_search_rf = model_selection.GridSearchCV(\n",
    "    estimator=ensemble.RandomForestClassifier(random_state=random_state),\n",
    "    param_grid=param_grid,\n",
    "    cv=n_splits,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "%time grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = grid_search_rf.predict(X_train)\n",
    "print(f'Train f1-score: {metrics.f1_score(y_train, y_train_pred):.2f}')\n",
    "\n",
    "y_test_pred = grid_search_rf.predict(X_test)\n",
    "print(f'Test f1-score: {metrics.f1_score(y_test, y_test_pred):.2f}')\n",
    "\n",
    "print('Best parameters: ', grid_search_rf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 490 ms, sys: 211 ms, total: 701 ms\n",
      "Wall time: 2min 28s\n",
      "Train f1-score: 0.84\n",
      "Test f1-score: 0.80\n",
      "Best parameters:  {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.34}\n"
     ]
    }
   ],
   "source": [
    "# Total combinations: 1 * 2 * 10 + 1 * 6 * 10 = 80\n",
    "param_distributions = [\n",
    "    {\n",
    "        'penalty': ['l1'],\n",
    "        'solver': ['liblinear', 'saga'],\n",
    "        'C': list(np.linspace(0.01, 1.0, 10, dtype=float))\n",
    "    },\n",
    "    {\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['liblinear', 'lbfgs', 'newton-cg', 'newton-cholesky', 'saga', 'sag'],\n",
    "        'C': list(np.linspace(0.01, 1.0, 10, dtype=float))\n",
    "    }\n",
    "]\n",
    "\n",
    "random_search_lr = model_selection.RandomizedSearchCV(\n",
    "    estimator=linear_model.LogisticRegression(random_state=random_state, max_iter=max_iter),\n",
    "    param_distributions=param_distributions,\n",
    "    cv=n_splits,\n",
    "    n_iter=20,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "%time random_search_lr.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = random_search_lr.predict(X_train)\n",
    "print(f'Train f1-score: {metrics.f1_score(y_train, y_train_pred):.2f}')\n",
    "\n",
    "y_test_pred = random_search_lr.predict(X_test)\n",
    "print(f'Test f1-score: {metrics.f1_score(y_test, y_test_pred):.2f}')\n",
    "\n",
    "print('Best parameters: ', random_search_lr.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.49 s, sys: 132 ms, total: 2.62 s\n",
      "Wall time: 17.6 s\n",
      "Train f1-score: 0.94\n",
      "Test f1-score: 0.83\n",
      "Best parameters:  {'n_estimators': 200, 'min_samples_leaf': 5, 'max_depth': 50}\n"
     ]
    }
   ],
   "source": [
    "# Total combinations: 5 * 2 * 10 = 100\n",
    "param_distributions = {\n",
    "    'n_estimators': list(range(80, 201, 30)),\n",
    "    'min_samples_leaf': [5, 10],\n",
    "    'max_depth': list(np.linspace(10, 50, 10, dtype=int))\n",
    "}\n",
    "\n",
    "random_search_rf = model_selection.RandomizedSearchCV(\n",
    "    estimator=ensemble.RandomForestClassifier(random_state=random_state),\n",
    "    param_distributions=param_distributions,\n",
    "    cv=n_splits,\n",
    "    n_iter=20,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "%time random_search_rf.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = random_search_rf.predict(X_train)\n",
    "print(f'Train f1-score: {metrics.f1_score(y_train, y_train_pred):.2f}')\n",
    "\n",
    "y_test_pred = random_search_rf.predict(X_test)\n",
    "print(f'Test f1-score: {metrics.f1_score(y_test, y_test_pred):.2f}')\n",
    "\n",
    "print('Best parameters: ', random_search_rf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-Structured Parzen Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [16:45<00:00, 50.29s/trial, best loss: -0.7866466124784773]\n",
      "Best values of hyper parameters:  {'C': 0.05084775379720359, 'classifier': 0, 'hyper_param_groups': 0, 'penalty_block1': 0, 'solver_block1': 0}\n"
     ]
    }
   ],
   "source": [
    "space = hp.choice('classifier',[{\n",
    "    'param':\n",
    "    {\n",
    "        'hyper_param_groups': hp.choice('hyper_param_groups', [\n",
    "        {\n",
    "            'penalty': hp.choice('penalty_block1', ['l2']),\n",
    "            'solver': hp.choice('solver_block1', ['newton-cg', 'sag', 'saga', 'lbfgs', 'liblinear'])\n",
    "        },\n",
    "        {\n",
    "            'penalty': hp.choice('penalty_block2', ['l1']),\n",
    "            'solver': hp.choice('solver_block2', ['saga', 'liblinear'])\n",
    "        },]),    \n",
    "        'C': hp.uniform('C', 0.01, 1)\n",
    "    }\n",
    "}])\n",
    "\n",
    "\n",
    "def hyperopt_lr(params, cv=n_splits, X=X_train, y=y_train, random_state=random_state):\n",
    "    params = {\n",
    "        'penalty': params['param']['hyper_param_groups']['penalty'],\n",
    "        'solver': params['param']['hyper_param_groups']['solver'],\n",
    "        'C': float(params['param']['C'])\n",
    "    }\n",
    "    \n",
    "    model = linear_model.LogisticRegression(**params, random_state=random_state, max_iter=max_iter)\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    score = model_selection.cross_val_score(model, X, y, cv=cv, scoring=\"f1\", n_jobs=-1).mean()\n",
    "    \n",
    "    return -score\n",
    "\n",
    "\n",
    "trials = hyperopt.Trials()\n",
    "\n",
    "best = hyperopt.fmin(\n",
    "    hyperopt_lr, \n",
    "    space=space, \n",
    "    algo=hyperopt.tpe.suggest, \n",
    "    max_evals=20, \n",
    "    trials=trials, \n",
    "    rstate=np.random.default_rng(random_state)\n",
    ")\n",
    "\n",
    "print('Best values of hyper parameters: ', best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1-score: 0.84\n",
      "Test f1-score: 0.79\n"
     ]
    }
   ],
   "source": [
    "# Use the best 'penalty' and 'solver' parameters from the output above.\n",
    "lr_model = linear_model.LogisticRegression(\n",
    "    penalty='l2',\n",
    "    solver='newton-cg',\n",
    "    C=float(best['C']),\n",
    "    random_state=random_state,\n",
    "    max_iter=max_iter\n",
    ")\n",
    "\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = lr_model.predict(X_train)\n",
    "print(f'Train f1-score: {metrics.f1_score(y_train, y_train_pred):.2f}')\n",
    "\n",
    "y_test_pred = lr_model.predict(X_test)\n",
    "print(f'Test f1-score: {metrics.f1_score(y_test, y_test_pred):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:56<00:00,  2.84s/trial, best loss: -0.8097293050428462]\n",
      "Best values of hyper parameters:  {'max_depth': 18.0, 'min_samples_leaf': 2.0, 'n_estimators': 103.0}\n",
      "Train f1-score: 0.99\n",
      "Test f1-score: 0.83\n"
     ]
    }
   ],
   "source": [
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 200, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 15, 26, 1),\n",
    "    'min_samples_leaf': hp.quniform('min_samples_leaf', 2, 10, 1)\n",
    "}\n",
    "\n",
    "\n",
    "def hyperopt_rf(params, cv=n_splits, X=X_train, y=y_train, random_state=random_state):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']),\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'min_samples_leaf': int(params['min_samples_leaf'])\n",
    "    }\n",
    "    \n",
    "    model = ensemble.RandomForestClassifier(**params, random_state=random_state)\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    score = model_selection.cross_val_score(model, X, y, cv=cv, scoring=\"f1\", n_jobs=-1).mean()\n",
    " \n",
    "    return -score\n",
    "\n",
    "\n",
    "trials = hyperopt.Trials()\n",
    "\n",
    "best = hyperopt.fmin(\n",
    "    hyperopt_rf, \n",
    "    space=space, \n",
    "    algo=hyperopt.tpe.suggest, \n",
    "    max_evals=20, \n",
    "    trials=trials, \n",
    "    rstate=np.random.default_rng(random_state)\n",
    ")\n",
    "\n",
    "print('Best values of hyper parameters: ', best)\n",
    "\n",
    "rf_model = ensemble.RandomForestClassifier(\n",
    "    random_state=random_state,\n",
    "    n_estimators=int(best['n_estimators']),\n",
    "    max_depth=int(best['max_depth']),\n",
    "    min_samples_leaf=int(best['min_samples_leaf'])\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = rf_model.predict(X_train)\n",
    "print(f'Train f1-score: {metrics.f1_score(y_train, y_train_pred):.2f}')\n",
    "\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "print(f'Test f1-score: {metrics.f1_score(y_test, y_test_pred):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-08 21:54:29,238] A new study created in memory with name: LogisticRegression\n",
      "[I 2024-07-08 21:54:51,100] Trial 0 finished with value: 0.7728712460678221 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.6433202420708939}. Best is trial 0 with value: 0.7728712460678221.\n",
      "[I 2024-07-08 21:54:52,154] Trial 1 finished with value: 0.7813923174428822 and parameters: {'penalty': 'l2', 'solver': 'liblinear', 'C': 0.15335897571711776}. Best is trial 1 with value: 0.7813923174428822.\n",
      "[I 2024-07-08 21:54:55,879] Trial 2 finished with value: 0.7731597976583198 and parameters: {'penalty': 'l2', 'solver': 'newton-cholesky', 'C': 0.43122816915861417}. Best is trial 1 with value: 0.7813923174428822.\n",
      "[I 2024-07-08 21:54:56,957] Trial 3 finished with value: 0.7741190721631985 and parameters: {'penalty': 'l2', 'solver': 'liblinear', 'C': 0.6198621611956622}. Best is trial 1 with value: 0.7813923174428822.\n",
      "[I 2024-07-08 21:54:58,115] Trial 4 finished with value: 0.7807690405229106 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.09844596060681662}. Best is trial 1 with value: 0.7813923174428822.\n",
      "[I 2024-07-08 21:55:01,274] Trial 5 finished with value: 0.7735973554152101 and parameters: {'penalty': 'l2', 'solver': 'newton-cholesky', 'C': 0.7109579135033541}. Best is trial 1 with value: 0.7813923174428822.\n",
      "[I 2024-07-08 21:55:14,049] Trial 6 finished with value: 0.7790638165480044 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.2025922492204165}. Best is trial 1 with value: 0.7813923174428822.\n",
      "[I 2024-07-08 21:55:14,702] Trial 7 finished with value: 0.7821065043042743 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.11705843871326001}. Best is trial 7 with value: 0.7821065043042743.\n",
      "[I 2024-07-08 21:55:25,392] Trial 8 finished with value: 0.7801531648109894 and parameters: {'penalty': 'l2', 'solver': 'saga', 'C': 0.08387200632287503}. Best is trial 7 with value: 0.7821065043042743.\n",
      "[I 2024-07-08 21:55:26,148] Trial 9 finished with value: 0.7732774435174282 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.40392993466634813}. Best is trial 7 with value: 0.7821065043042743.\n",
      "[I 2024-07-08 21:55:27,253] Trial 10 finished with value: 0.769297917592523 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.9650294269249222}. Best is trial 7 with value: 0.7821065043042743.\n",
      "[I 2024-07-08 21:55:27,717] Trial 11 finished with value: 0.7775856578216749 and parameters: {'penalty': 'l2', 'solver': 'liblinear', 'C': 0.26592735340064905}. Best is trial 7 with value: 0.7821065043042743.\n",
      "[I 2024-07-08 21:55:28,156] Trial 12 finished with value: 0.7837930859532589 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.016256134639307612}. Best is trial 12 with value: 0.7837930859532589.\n",
      "[I 2024-07-08 21:55:28,568] Trial 13 finished with value: 0.7795010517461802 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.013806006770720802}. Best is trial 12 with value: 0.7837930859532589.\n",
      "[I 2024-07-08 21:55:29,452] Trial 14 finished with value: 0.7774953853366497 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.28883040927074954}. Best is trial 12 with value: 0.7837930859532589.\n",
      "[I 2024-07-08 21:55:29,930] Trial 15 finished with value: 0.7836710253135878 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.017524027067084065}. Best is trial 12 with value: 0.7837930859532589.\n",
      "[I 2024-07-08 21:55:30,427] Trial 16 finished with value: 0.7773248833951166 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.01126676206345721}. Best is trial 12 with value: 0.7837930859532589.\n",
      "[I 2024-07-08 21:55:43,013] Trial 17 finished with value: 0.776576502286678 and parameters: {'penalty': 'l2', 'solver': 'saga', 'C': 0.3039647703517029}. Best is trial 12 with value: 0.7837930859532589.\n",
      "[I 2024-07-08 21:55:44,022] Trial 18 finished with value: 0.7711126453410305 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.8210626779556947}. Best is trial 12 with value: 0.7837930859532589.\n",
      "[I 2024-07-08 21:55:44,971] Trial 19 finished with value: 0.7728452961857217 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.5115306824090354}. Best is trial 12 with value: 0.7837930859532589.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best values of hyper parameters:  {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.016256134639307612}\n",
      "Train f1-score:  0.78\n",
      "Test f1-score: 0.81\n"
     ]
    }
   ],
   "source": [
    "def optuna_lr(trial):\n",
    "    penalty = trial.suggest_categorical('penalty', ['l2'])\n",
    "    solver = trial.suggest_categorical('solver', ['newton-cg', 'sag', 'saga', 'lbfgs', 'liblinear', 'newton-cholesky'])\n",
    "    C = trial.suggest_float('C', low=0.01, high=1)\n",
    "    \n",
    "    model = linear_model.LogisticRegression(\n",
    "        penalty=penalty,\n",
    "        solver=solver,\n",
    "        C=C,\n",
    "        random_state=random_state,\n",
    "        max_iter=max_iter\n",
    "    )\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits=n_splits)\n",
    "    score = model_selection.cross_val_score(model, X_train, y_train, cv=kf, scoring=\"f1\", n_jobs=-1).mean()\n",
    " \n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(study_name='LogisticRegression', direction='maximize')\n",
    "\n",
    "study.optimize(optuna_lr, n_trials=20)\n",
    "\n",
    "print('Best values of hyper parameters: ', study.best_params)\n",
    "print('Train f1-score: ', round(study.best_value, 2))\n",
    "\n",
    "model = linear_model.LogisticRegression(**study.best_params, random_state=random_state, max_iter=max_iter)\n",
    "model.fit(X_train, y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "print(f'Test f1-score: {metrics.f1_score(y_test, y_test_pred):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Foresst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-08 20:44:52,313] A new study created in memory with name: RandomForestClassifier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.29 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-08 20:44:55,024] Trial 0 finished with value: 0.8032959630485156 and parameters: {'n_estimators': 260, 'max_depth': 38, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.8032959630485156.\n",
      "[I 2024-07-08 20:44:57,755] Trial 1 finished with value: 0.8017659784937461 and parameters: {'n_estimators': 270, 'max_depth': 35, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.8032959630485156.\n",
      "[I 2024-07-08 20:44:58,860] Trial 2 finished with value: 0.7958124144308217 and parameters: {'n_estimators': 110, 'max_depth': 39, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.8032959630485156.\n",
      "[I 2024-07-08 20:45:00,140] Trial 3 finished with value: 0.7975724733176891 and parameters: {'n_estimators': 130, 'max_depth': 20, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.8032959630485156.\n",
      "[I 2024-07-08 20:45:02,056] Trial 4 finished with value: 0.7971045600656945 and parameters: {'n_estimators': 210, 'max_depth': 23, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.8032959630485156.\n",
      "[I 2024-07-08 20:45:03,189] Trial 5 finished with value: 0.8021085646286472 and parameters: {'n_estimators': 120, 'max_depth': 35, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.8032959630485156.\n",
      "[I 2024-07-08 20:45:06,222] Trial 6 finished with value: 0.8027586196669778 and parameters: {'n_estimators': 300, 'max_depth': 31, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.8032959630485156.\n",
      "[I 2024-07-08 20:45:07,602] Trial 7 finished with value: 0.8070169226864914 and parameters: {'n_estimators': 140, 'max_depth': 21, 'min_samples_leaf': 6}. Best is trial 7 with value: 0.8070169226864914.\n",
      "[I 2024-07-08 20:45:09,959] Trial 8 finished with value: 0.8022851253620763 and parameters: {'n_estimators': 280, 'max_depth': 30, 'min_samples_leaf': 6}. Best is trial 7 with value: 0.8070169226864914.\n",
      "[I 2024-07-08 20:45:11,315] Trial 9 finished with value: 0.8020222082595057 and parameters: {'n_estimators': 130, 'max_depth': 25, 'min_samples_leaf': 4}. Best is trial 7 with value: 0.8070169226864914.\n",
      "[I 2024-07-08 20:45:12,944] Trial 10 finished with value: 0.798061979017714 and parameters: {'n_estimators': 180, 'max_depth': 15, 'min_samples_leaf': 5}. Best is trial 7 with value: 0.8070169226864914.\n",
      "[I 2024-07-08 20:45:15,447] Trial 11 finished with value: 0.8051723544218967 and parameters: {'n_estimators': 230, 'max_depth': 19, 'min_samples_leaf': 3}. Best is trial 7 with value: 0.8070169226864914.\n",
      "[I 2024-07-08 20:45:17,512] Trial 12 finished with value: 0.805990835028809 and parameters: {'n_estimators': 210, 'max_depth': 17, 'min_samples_leaf': 3}. Best is trial 7 with value: 0.8070169226864914.\n",
      "[I 2024-07-08 20:45:19,251] Trial 13 finished with value: 0.8063317013892009 and parameters: {'n_estimators': 170, 'max_depth': 15, 'min_samples_leaf': 3}. Best is trial 7 with value: 0.8070169226864914.\n",
      "[I 2024-07-08 20:45:20,752] Trial 14 finished with value: 0.8009777221004732 and parameters: {'n_estimators': 170, 'max_depth': 23, 'min_samples_leaf': 6}. Best is trial 7 with value: 0.8070169226864914.\n",
      "[I 2024-07-08 20:45:22,205] Trial 15 finished with value: 0.7963393019643856 and parameters: {'n_estimators': 160, 'max_depth': 15, 'min_samples_leaf': 5}. Best is trial 7 with value: 0.8070169226864914.\n",
      "[I 2024-07-08 20:45:23,789] Trial 16 finished with value: 0.8057641607617088 and parameters: {'n_estimators': 150, 'max_depth': 20, 'min_samples_leaf': 6}. Best is trial 7 with value: 0.8070169226864914.\n",
      "[I 2024-07-08 20:45:25,455] Trial 17 finished with value: 0.8017791650623762 and parameters: {'n_estimators': 180, 'max_depth': 26, 'min_samples_leaf': 5}. Best is trial 7 with value: 0.8070169226864914.\n",
      "[I 2024-07-08 20:45:26,596] Trial 18 finished with value: 0.8029960872080825 and parameters: {'n_estimators': 100, 'max_depth': 18, 'min_samples_leaf': 3}. Best is trial 7 with value: 0.8070169226864914.\n",
      "[I 2024-07-08 20:45:27,929] Trial 19 finished with value: 0.8044521230123651 and parameters: {'n_estimators': 150, 'max_depth': 22, 'min_samples_leaf': 6}. Best is trial 7 with value: 0.8070169226864914.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best values of hyper parameters:  {'n_estimators': 140, 'max_depth': 21, 'min_samples_leaf': 6}\n",
      "Train f1-score:  0.81\n",
      "Test f1-score: 0.83\n"
     ]
    }
   ],
   "source": [
    "def optuna_rf(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', low=100, high=300, step=10)\n",
    "    max_depth = trial.suggest_int('max_depth', low=15, high=40, step=1)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', low=3, high=7, step=1)\n",
    "    \n",
    "    model = ensemble.RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits=n_splits)\n",
    "    score = model_selection.cross_val_score(model, X_train, y_train, cv=kf, scoring=\"f1\", n_jobs=-1).mean()\n",
    "    \n",
    "    return score\n",
    "\n",
    "%time\n",
    "\n",
    "study = optuna.create_study(study_name='RandomForestClassifier', direction='maximize')\n",
    "\n",
    "study.optimize(optuna_rf, n_trials=20)\n",
    "\n",
    "print('Best values of hyper parameters: ', study.best_params)\n",
    "print('Train f1-score: ', round(study.best_value, 2))\n",
    "\n",
    "model = ensemble.RandomForestClassifier(**study.best_params, random_state=random_state)\n",
    "model.fit(X_train, y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "print(f'Test f1-score: {metrics.f1_score(y_test, y_test_pred):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following results have been observed using the applied hyper parameter optimization methods.\n",
    "\n",
    "| Model Type | F1-Score on Train Data | F1-Score on Test Data |\n",
    "| ---------- | ---------------------- | --------------------- |\n",
    "| No optimization - Logistic Regression | 0,89 | 0,78 |\n",
    "| No optimization - Random Forest | 1 | 0,81 |\n",
    "| Grid Search - Logistic Regression | 0,84 | 0,80 |\n",
    "| Grid Search - Random Forest | 0,94 | 0,83 |\n",
    "| Randomised Search - Logistic Regression | 0,84 | 0,80 |\n",
    "| Randomised Search - Random Forest | 0,94 | 0,83 |\n",
    "| Hyperopt - Logistic Regression | 0,84 | 0,79 |\n",
    "| Hyperopt - Random Forest | 0,99 | 0,83 |\n",
    "| Optuna - Logistic Regression | 0,78 | 0,81 |\n",
    "| Optuna - Random Forest | 0,81 | 0,83 |\n",
    "\n",
    "We an conclude that Random Forest is showing on average better restults than the Logistic Regression, although it tends to overfitting. \n",
    "\n",
    "For Logistic Regression all optimization methods showed better results than the basic model. Of which Optuna performed the best. \n",
    "\n",
    "For Random Forest all optimization methods showed better results than the basic model. Of which only Optuna did not tend to overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
