{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u64t6tS-K97g"
   },
   "source": [
    "# Practise YOLOv5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUmmo7EHRpci"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8D5Xw0ZeRsOW"
   },
   "outputs": [],
   "source": [
    "# Libraries to load and analyze data\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import glob\n",
    "import numpy as np\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn import preprocessing\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Libraries to work with images and build models\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image, ImageDraw\n",
    "from albumentations import (HorizontalFlip, ShiftScaleRotate, VerticalFlip, Normalize,Flip,\n",
    "                            Compose, GaussNoise)\n",
    "import cv2\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "device = 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "dataset_path = 'data/VOCdevkit/VOC2012/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading only annotations for the images made in the year 2010 as pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAFtNVtnLWqy"
   },
   "outputs": [],
   "source": [
    "def xml_to_csv(path=dataset_path+'Annotations/'):\n",
    "    xml_list = []\n",
    "    for xml_file in tqdm(glob.glob(path + '/2010*.xml')):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for obj in root.findall('object'):\n",
    "            bbx = obj.find('bndbox')\n",
    "            xmin = int(bbx.find('xmin').text)\n",
    "            ymin = int(bbx.find('ymin').text)\n",
    "            xmax = int(bbx.find('xmax').text)\n",
    "            ymax = int(bbx.find('ymax').text)\n",
    "            label = obj.find('name').text\n",
    "\n",
    "            value = (root.find('filename').text,\n",
    "                     int(root.find('size')[0].text),\n",
    "                     int(root.find('size')[2].text),\n",
    "                     int(root.find('size')[1].text),\n",
    "                     label,\n",
    "                     xmin,\n",
    "                     ymin,\n",
    "                     xmax,\n",
    "                     ymax\n",
    "                     )\n",
    "            xml_list.append(value)\n",
    "    column_name = ['filename', 'channels', 'width', 'height',\n",
    "                   'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "    return xml_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8REgqM_8LYZ8",
    "outputId": "8798f6af-e9d0-4c9f-d84f-d3bf7c0916d9"
   },
   "outputs": [],
   "source": [
    "xml_df = xml_to_csv()\n",
    "xml_df.to_csv('VOC_2010.csv', index=None)\n",
    "print('Saved as csv file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easier troubleshooting I keep only the top 100 objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wn-aNs-nLcOB"
   },
   "outputs": [],
   "source": [
    "xml_df = xml_df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "LOv1kVMLkjLr",
    "outputId": "07f49aa7-4966-4aee-aa73-50fbe976fbbf"
   },
   "outputs": [],
   "source": [
    "xml_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DBryk78TLrj2",
    "outputId": "b17c640b-56af-4675-e262-edfd85bce772"
   },
   "outputs": [],
   "source": [
    "print(f'Oveall number of objects: {xml_df.shape[0]}')\n",
    "print(f'Number of images in the dataframe: {len(np.unique(xml_df[\"filename\"]))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2T17NtqLzGw"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBjTXglfL2FL"
   },
   "source": [
    "Encode object classes with `LabelEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0moCq9HMLzk7"
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "xml_df['class'] = le.fit_transform(xml_df['class'])\n",
    "xml_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amBRdldYkn4T"
   },
   "source": [
    "Save the encoding as dictionary in the `mappring` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRkj--dqj2OM"
   },
   "outputs": [],
   "source": [
    "class_to_id_mapping = dict(zip(le.classes_, range(len(le.classes_))))\n",
    "class_to_id_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuBNUWfiMGSF"
   },
   "source": [
    "Now we need to convert our data to the **YOLOv5** format.\n",
    "\n",
    "The annotation for each object should contain the following information:\n",
    "\n",
    "`Class X Y Width Height`\n",
    "\n",
    "For this, we'll use the helper function `convert_to_yolov5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEoYc3MSN_iG"
   },
   "outputs": [],
   "source": [
    "def convert_to_yolov5(df, unique_img_names):\n",
    "    df_array = np.array(df)\n",
    "\n",
    "    # For each unique image name\n",
    "    for unique_img_name in unique_img_names:\n",
    "        print_buffer = []\n",
    "\n",
    "        # For each object\n",
    "        for i in range(len(df)):\n",
    "            point =  df_array[i]\n",
    "            img_name = point[0]\n",
    "\n",
    "            if img_name == unique_img_name:\n",
    "                class_id = point[4]\n",
    "\n",
    "                # Transform bbox coordinates\n",
    "                X = (point[5] + point[7]) / 2\n",
    "                Y = (point[6] + point[8]) / 2\n",
    "                Width = (point[7] - point[5])\n",
    "                Height = (point[8] - point[6])\n",
    "\n",
    "                # Normalize coordinates\n",
    "                image_w, image_h = point[2], point[3]\n",
    "                X /= image_w\n",
    "                Y /= image_h\n",
    "                Width /= image_w\n",
    "                Height /= image_h\n",
    "              \n",
    "                # Save the bbox information to the buffer\n",
    "                print_buffer.append(\"{} {:.3f} {:.3f} {:.3f} {:.3f}\".format(class_id, X, Y, Width, Height))\n",
    "\n",
    "        # Get file names to save the annotation\n",
    "        save_file_name = os.path.join(dataset_path+'JPEGImages/yolov5/labels/', unique_img_name.replace(\"jpg\", \"txt\"))\n",
    "        # Save the annotations to disk\n",
    "        print(\"\\n\".join(print_buffer), file=open(save_file_name, \"w\"))\n",
    "        # Copy the image to the new folder                \n",
    "        shutil.copy(dataset_path+'JPEGImages/'+unique_img_name, dataset_path+'JPEGImages/yolov5/images/'+unique_img_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K2SbYb9scEqx",
    "outputId": "e63d3a04-a14d-405a-fd3c-70e8cfdbc8de"
   },
   "outputs": [],
   "source": [
    "# First let's get the unique image names\n",
    "unique_img_names = xml_df['filename'].unique()\n",
    "print('Unique image names number: ', len(unique_img_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMJdgDViGo-1"
   },
   "outputs": [],
   "source": [
    "# Then we convert the annotations to YOLOv5 format\n",
    "convert_to_yolov5(xml_df, unique_img_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvgPfHaUfZ2x"
   },
   "source": [
    "Let's test the transformed annotations via a viaualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAii5lYvjc7V"
   },
   "outputs": [],
   "source": [
    "id_to_class_mapping = dict(zip(class_to_id_mapping.values(), class_to_id_mapping.keys()))\n",
    "id_to_class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3a74rD2eGPQ"
   },
   "outputs": [],
   "source": [
    "def plot_bounding_box(image, annotation_list):\n",
    "    annotations = np.array(annotation_list)\n",
    "    w, h = image.size\n",
    "\n",
    "    plotted_image = ImageDraw.Draw(image)\n",
    "\n",
    "    transformed_annotations = np.copy(annotations)\n",
    "    transformed_annotations[:,[1,3]] = annotations[:,[1,3]] * w\n",
    "    transformed_annotations[:,[2,4]] = annotations[:,[2,4]] * h \n",
    "    \n",
    "    transformed_annotations[:,1] = transformed_annotations[:,1] - (transformed_annotations[:,3] / 2)\n",
    "    transformed_annotations[:,2] = transformed_annotations[:,2] - (transformed_annotations[:,4] / 2)\n",
    "    transformed_annotations[:,3] = transformed_annotations[:,1] + transformed_annotations[:,3]\n",
    "    transformed_annotations[:,4] = transformed_annotations[:,2] + transformed_annotations[:,4]\n",
    "    \n",
    "    for ann in transformed_annotations:\n",
    "        obj_cls, x0, y0, x1, y1 = ann\n",
    "        plotted_image.rectangle(((x0,y0), (x1,y1)), outline=\"#FF0000FF\", width=2)\n",
    "\n",
    "        plotted_image.text((x0, y0 - 10), id_to_class_mapping[(int(obj_cls))], fill=\"#000\")\n",
    "    \n",
    "    \n",
    "    plt.imshow(np.array(image))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ke65a2EUes4e"
   },
   "outputs": [],
   "source": [
    "# Pick a random annotation file \n",
    "annotation_random = !ls {dataset_path}/JPEGImages/yolov5/*txt | gshuf -n 1\n",
    "annotation_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "XjmIDka5ebyt",
    "outputId": "b778f82b-0e9b-4f57-b817-2081fe0e7065"
   },
   "outputs": [],
   "source": [
    "# Load random annotation file\n",
    "annotation_file = annotation_random[0]\n",
    "with open(annotation_file, \"r\") as file:\n",
    "    annotation_list = file.read().split(\"\\n\")[:-1]\n",
    "    annotation_list = [x.split(\" \") for x in annotation_list]\n",
    "    annotation_list = [[float(y) for y in x ] for x in annotation_list]\n",
    "\n",
    "#Get the corresponding image file\n",
    "image_file = annotation_file.replace(\"txt\", \"jpg\")\n",
    "assert os.path.exists(image_file)\n",
    "\n",
    "#Load the image\n",
    "image = Image.open(image_file)\n",
    "\n",
    "#Plot the Bounding Box\n",
    "plot_bounding_box(image, annotation_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TljEYre_1GIq"
   },
   "source": [
    "Trained model is available in the `hub` class of the `torch` library.\n",
    "\n",
    "Let's see what would be the model predictions for our random image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457,
     "referenced_widgets": [
      "8a49489725b6434889e60368e2380635",
      "27882cde128d4e92b8621c474a592fbf",
      "dd12dc276d784fd882f6c5a88be1bf5b",
      "c6bf9daa3b40463e82d88d5f3fa7024c",
      "d79cd9fa921847b58b42a147aa808523",
      "b70e1703b7a24fdf8b53f7c4f972df12",
      "03bed50e8e424125a1a94bfb29e95d9e",
      "536fae0cc9044c2daa224dfeb367d461",
      "30ca7602a5fd42c3b261de21d8c6aa17",
      "daece13ba3d0434190868b1e601983ea",
      "ad9993e2585d413aa165112fc357e741"
     ]
    },
    "id": "znFlwjx1x7ij",
    "outputId": "99c0b3b3-a166-43d4-90da-74cc53381ea6"
   },
   "outputs": [],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "model.model.to(device)\n",
    "print(next(model.model.parameters()).device)\n",
    "img = annotation_random[0].replace(\"txt\", \"jpg\")\n",
    "\n",
    "# Inference\n",
    "results = model(img)\n",
    "\n",
    "# Results\n",
    "results.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train YOLOv5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDIuUhVqzGKw"
   },
   "source": [
    "Train the **yolov5** model using the prepared `train.py` file from the repository https://github.com/ultralytics/yolov5.git.\n",
    "\n",
    "For this model, images and object classes should be located in the images and labels folders, respectively.\n",
    "\n",
    "Test the model on the validation set and display the resulting images with bbox and object classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the `coco128` dataset and a predefined YOLOv5 Medium size model with weights.\n",
    "\n",
    "Not to overload the machine, I am using a small batch of 4 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tcqbGZWr082T"
   },
   "outputs": [],
   "source": [
    "!python train.py --img 640 --batch 4 --epochs 10 --data coco128.yaml --weights yolov5m.pt --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has achieved the mAP=0.86 after 10 training epochs, which is a very good result.\n",
    "\n",
    "Here is a summary of metrics for training and validation:\n",
    "![Training and validation results](results.png)\n",
    "\n",
    "Here some images with boxes and class labels from the validation process:\n",
    "\n",
    "Image 1\n",
    "![Image 1](val_batch0_pred.jpg)\n",
    "\n",
    "Image 2\n",
    "![Image 2](val_batch1_pred.jpg)\n",
    "\n",
    "Image 3\n",
    "![Image 3](val_batch2_pred.jpg)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Модуль_12_YOLO_дз.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
